{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch math"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG7PwIXnIvd9",
        "outputId": "c9a1c24e-8afd-46e4-9c44-6fe7e485dcf8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement math (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for math\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile moht_components.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class MHAAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention: num_q_heads = num_kv_heads\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_head = config.n_head\n",
        "        self.d_head = config.n_embd // config.n_head\n",
        "        self.dropout = config.dropout\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        # Q, K, V projections for all heads\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Flash attention support\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Calculate Q, K, V for all heads\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, self.d_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, self.d_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, self.d_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        # Attention\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True\n",
        "            )\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_head))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class GQAAttention(nn.Module):\n",
        "    \"\"\"Grouped-Query Attention: num_q_heads > num_kv_heads\"\"\"\n",
        "    def __init__(self, config, num_kv_heads=2):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        assert config.n_head % num_kv_heads == 0, \"n_head must be divisible by num_kv_heads\"\n",
        "\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_head = config.n_head\n",
        "        self.num_kv_heads = num_kv_heads\n",
        "        self.d_head = config.n_embd // config.n_head\n",
        "        self.dropout = config.dropout\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        # Q projection for all heads\n",
        "        self.q_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # K, V projections for fewer heads\n",
        "        self.k_proj = nn.Linear(config.n_embd, num_kv_heads * self.d_head, bias=config.bias)\n",
        "        self.v_proj = nn.Linear(config.n_embd, num_kv_heads * self.d_head, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Flash attention support\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Q for all heads\n",
        "        q = self.q_proj(x).view(B, T, self.n_head, self.d_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        # K, V for fewer heads\n",
        "        k = self.k_proj(x).view(B, T, self.num_kv_heads, self.d_head).transpose(1, 2)  # (B, num_kv_heads, T, hs)\n",
        "        v = self.v_proj(x).view(B, T, self.num_kv_heads, self.d_head).transpose(1, 2)  # (B, num_kv_heads, T, hs)\n",
        "\n",
        "        # Repeat K, V to match number of Q heads\n",
        "        # Each KV head is shared across n_head // num_kv_heads query heads\n",
        "        k = k.repeat_interleave(self.n_head // self.num_kv_heads, dim=1)  # (B, nh, T, hs)\n",
        "        v = v.repeat_interleave(self.n_head // self.num_kv_heads, dim=1)  # (B, nh, T, hs)\n",
        "\n",
        "        # Attention\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True\n",
        "            )\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_head))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class MQAAttention(nn.Module):\n",
        "    \"\"\"Multi-Query Attention: num_kv_heads = 1\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_head = config.n_head\n",
        "        self.d_head = config.n_embd // config.n_head\n",
        "        self.dropout = config.dropout\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        # Q projection for all heads\n",
        "        self.q_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # K, V projections for single head\n",
        "        self.k_proj = nn.Linear(config.n_embd, self.d_head, bias=config.bias)\n",
        "        self.v_proj = nn.Linear(config.n_embd, self.d_head, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Flash attention support\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Q for all heads\n",
        "        q = self.q_proj(x).view(B, T, self.n_head, self.d_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        # K, V for single head\n",
        "        k = self.k_proj(x).view(B, T, 1, self.d_head).transpose(1, 2)  # (B, 1, T, hs)\n",
        "        v = self.v_proj(x).view(B, T, 1, self.d_head).transpose(1, 2)  # (B, 1, T, hs)\n",
        "\n",
        "        # Repeat K, V to match number of Q heads\n",
        "        k = k.repeat(1, self.n_head, 1, 1)  # (B, nh, T, hs)\n",
        "        v = v.repeat(1, self.n_head, 1, 1)  # (B, nh, T, hs)\n",
        "\n",
        "        # Attention\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True\n",
        "            )\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_head))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class StaticMoASAttention(nn.Module):\n",
        "    \"\"\"Static Mixture of Attention Schemes - averages MHA, GQA, MQA outputs\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mha = MHAAttention(config)\n",
        "        self.gqa = GQAAttention(config, num_kv_heads=2)\n",
        "        self.mqa = MQAAttention(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        o_mha = self.mha(x)\n",
        "        o_gqa = self.gqa(x)\n",
        "        o_mqa = self.mqa(x)\n",
        "        return (o_mha + o_gqa + o_mqa) / 3.0\n",
        "\n",
        "\n",
        "class MoASAttention(nn.Module):\n",
        "    \"\"\"Mixture of Attention Schemes with learned per-token routing\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        # Three attention branches\n",
        "        self.mha = MHAAttention(config)\n",
        "        self.gqa = GQAAttention(config, num_kv_heads=2)\n",
        "        self.mqa = MQAAttention(config)\n",
        "\n",
        "        # Router: 2-layer MLP\n",
        "        router_hidden = config.n_embd // 4\n",
        "        self.router = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, router_hidden, bias=config.bias),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(router_hidden, 3, bias=config.bias)  # 3 attention types\n",
        "        )\n",
        "\n",
        "        self.gate_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, return_gate_stats=False):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Compute all attention outputs\n",
        "        o_mha = self.mha(x)  # (B, T, C)\n",
        "        o_gqa = self.gqa(x)  # (B, T, C)\n",
        "        o_mqa = self.mqa(x)  # (B, T, C)\n",
        "\n",
        "        # Stack outputs: (B, T, 3, C)\n",
        "        outputs = torch.stack([o_mha, o_gqa, o_mqa], dim=2)\n",
        "\n",
        "        # Compute routing logits for each token\n",
        "        router_logits = self.router(x)  # (B, T, 3)\n",
        "        gates = F.softmax(router_logits, dim=-1)  # (B, T, 3)\n",
        "        gates = self.gate_dropout(gates)\n",
        "\n",
        "        # Mix outputs per token: (B, T, 3, 1) * (B, T, 3, C) -> (B, T, 3, C) -> (B, T, C)\n",
        "        y = (gates.unsqueeze(-1) * outputs).sum(dim=2)\n",
        "\n",
        "        if return_gate_stats:\n",
        "            # Return average gate values for logging\n",
        "            avg_gates = gates.mean(dim=(0, 1))  # (3,)\n",
        "            return y, avg_gates\n",
        "\n",
        "        return y\n",
        "\n",
        "    def get_load_balancing_loss(self, x):\n",
        "        \"\"\"Compute load balancing loss to encourage using all attention types\"\"\"\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Compute gates\n",
        "        router_logits = self.router(x)  # (B, T, 3)\n",
        "        gates = F.softmax(router_logits, dim=-1)  # (B, T, 3)\n",
        "\n",
        "        # Average gate per type across all tokens\n",
        "        avg_gates = gates.mean(dim=(0, 1))  # (3,)\n",
        "\n",
        "        # Target: uniform distribution (1/3 for each type)\n",
        "        target = torch.ones_like(avg_gates) / 3.0\n",
        "\n",
        "        # MSE loss\n",
        "        loss = F.mse_loss(avg_gates, target)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyo4_VSoI6EA",
        "outputId": "e4736525-491b-433c-a8b9-37f4f31b094c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing moht_components.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, importlib\n",
        "\n",
        "# 1) Go to the folder where the file was written (Colab default)\n",
        "%cd /content\n",
        "\n",
        "# 2) Verify the file exists\n",
        "print(\"cwd:\", os.getcwd())\n",
        "print(\"has file:\", os.path.exists(\"moht_components.py\"))\n",
        "!ls -l moht_components.py\n",
        "\n",
        "# 3) Ensure /content is on Python path (usually already is)\n",
        "if \"/content\" not in sys.path:\n",
        "    sys.path.insert(0, \"/content\")\n",
        "\n",
        "# 4) Import + reload (useful if you edited the file and re-ran %%writefile)\n",
        "import moht_components\n",
        "importlib.reload(moht_components)\n",
        "\n",
        "from moht_components import StaticMoASAttention, MoASAttention\n",
        "print(\"Imported OK ✅\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SZezJz3KWgG",
        "outputId": "5d575ec8-7260-4a76-e903-116a4a60f3ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "cwd: /content\n",
            "has file: True\n",
            "-rw-r--r-- 1 root root 10421 Dec 16 07:51 moht_components.py\n",
            "Imported OK ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-gzjQDpIbFQ",
        "outputId": "49a96648-2eb9-4887-a99e-114f49348656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing moht_gpt.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile moht_gpt.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from dataclasses import dataclass\n",
        "from moht_components import StaticMoASAttention, MoASAttention\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "    attention_type: str = 'baseline'  # 'baseline', 'static_moas', 'moas'\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "\n",
        "        # Choose attention type\n",
        "        if config.attention_type == 'baseline':\n",
        "            self.attn = CausalSelfAttention(config)\n",
        "        elif config.attention_type == 'static_moas':\n",
        "            self.attn = StaticMoASAttention(config)\n",
        "        elif config.attention_type == 'moas':\n",
        "            self.attn = MoASAttention(config)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown attention type: {config.attention_type}\")\n",
        "\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
        "        # This behavior is deprecated and will be an error in future versions\"\n",
        "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def get_load_balancing_loss(self, idx):\n",
        "        \"\"\"Compute load balancing loss for MoAS attention\"\"\"\n",
        "        if self.config.attention_type != 'moas':\n",
        "            return torch.tensor(0.0, device=idx.device)\n",
        "\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "        # Forward to get embeddings\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "        # Accumulate load balancing loss from all layers\n",
        "        total_lb_loss = 0.0\n",
        "        for block in self.transformer.h:\n",
        "            x_norm = block.ln_1(x)\n",
        "            lb_loss = block.attn.get_load_balancing_loss(x_norm)\n",
        "            total_lb_loss += lb_loss\n",
        "            x = x + block.attn(x_norm)\n",
        "            x = x + block.mlp(block.ln_2(x))\n",
        "\n",
        "        return total_lb_loss / len(self.transformer.h)\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)} with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)} with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in torch.optim.AdamW.__init__.__code__.co_varnames\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import tiktoken\n",
        "from moht_gpt import GPT, GPTConfig\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "out_dir = 'out'\n",
        "eval_interval = 200\n",
        "log_interval = 10\n",
        "eval_iters = 20\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume'\n",
        "# data\n",
        "dataset = 'wikitext-2'\n",
        "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 256 # context of up to 256 tokens\n",
        "# model\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "attention_type = 'baseline' # 'baseline', 'static_moas', 'moas'\n",
        "load_balance_weight = 0.01 # weight for load balancing loss (only for 'moas')\n",
        "# adamw optimizer\n",
        "learning_rate = 1e-3 # max learning rate\n",
        "max_iters = 2000 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 100 # how many steps to warm up for\n",
        "lr_decay_iters = 2000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 1e-4 # minimum learning rate, should be ~= learning_rate/10\n",
        "# system\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Data Loading\n",
        "# -----------------------------------------------------------------------------\n",
        "def get_data():\n",
        "    # simple data loader for wikitext-2\n",
        "    # we will download it if it doesn't exist\n",
        "    data_dir = os.path.join('data', dataset)\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    input_file_path = os.path.join(data_dir, 'input.txt')\n",
        "    if not os.path.exists(input_file_path):\n",
        "        import requests\n",
        "        print(\"Downloading WikiText-2...\")\n",
        "        data_url = 'https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt'\n",
        "        try:\n",
        "            with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(requests.get(data_url).text)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download data: {e}\")\n",
        "            print(\"Creating dummy data instead.\")\n",
        "            with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"Hello world \" * 10000)\n",
        "\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    # tokenize (character level)\n",
        "    chars = sorted(list(set(data)))\n",
        "    vocab_size = len(chars)\n",
        "    print(f\"Vocab size: {vocab_size}\")\n",
        "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "    itos = { i:ch for i,ch in enumerate(chars) }\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "    train_ids = encode(data)\n",
        "    n = len(train_ids)\n",
        "    train_data = np.array(train_ids[:int(n*0.9)], dtype=np.uint16)\n",
        "    val_data = np.array(train_ids[int(n*0.9):], dtype=np.uint16)\n",
        "    return train_data, val_data, vocab_size\n",
        "\n",
        "train_data, val_data, vocab_size = get_data()\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Training Setup\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "# config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = contextlib.nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=vocab_size, dropout=dropout, attention_type=attention_type)\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "model.to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    # TODO: implement resume\n",
        "    pass\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Training Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "\n",
        "print(f\"Training on {device}...\")\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(local_iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if local_iter_num % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {local_iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if always_save_checkpoint:\n",
        "            if local_iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': local_iter_num,\n",
        "                    'best_val_loss': losses['val'],\n",
        "                    'config': config_keys,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "\n",
        "    if local_iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "\n",
        "            # Add load balancing loss for MoAS\n",
        "            if attention_type == 'moas':\n",
        "                lb_loss = model.get_load_balancing_loss(X)\n",
        "                loss = loss + load_balance_weight * lb_loss\n",
        "\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        # scaler.scale(loss).backward()\n",
        "        loss.backward()\n",
        "\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    optimizer.step()\n",
        "    # scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if local_iter_num % log_interval == 0:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        print(f\"iter {local_iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
        "\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if local_iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "print(\"Training finished!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egCEiH0GKYi8",
        "outputId": "c9805965-5c80-4f9c-f223-41e543bbb4d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Knuy3tvKrTR",
        "outputId": "a4820f2a-dbed-42c2-f381-4ca09475aa80"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading WikiText-2...\n",
            "Vocab size: 283\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "number of parameters: 10.73M\n",
            "num decayed parameter tensors: 26 with 10,823,808 parameters\n",
            "num non-decayed parameter tensors: 13 with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "Training on cuda...\n",
            "step 0: train loss 5.7300, val loss 5.7355\n",
            "iter 0: loss 5.7324, time 2219.06ms\n",
            "iter 10: loss 4.0670, time 93.62ms\n",
            "iter 20: loss 3.3951, time 94.22ms\n",
            "iter 30: loss 2.8527, time 94.16ms\n",
            "iter 40: loss 2.6811, time 94.81ms\n",
            "iter 50: loss 2.5449, time 94.47ms\n",
            "iter 60: loss 2.5387, time 95.35ms\n",
            "iter 70: loss 2.4884, time 94.39ms\n",
            "iter 80: loss 2.5052, time 96.19ms\n",
            "iter 90: loss 2.4781, time 96.51ms\n",
            "iter 100: loss 2.4915, time 95.68ms\n",
            "iter 110: loss 2.4626, time 96.09ms\n",
            "iter 120: loss 2.4603, time 97.39ms\n",
            "iter 130: loss 2.5099, time 96.21ms\n",
            "iter 140: loss 2.4814, time 100.34ms\n",
            "iter 150: loss 2.4404, time 97.87ms\n",
            "iter 160: loss 2.4058, time 96.47ms\n",
            "iter 170: loss 2.4540, time 95.88ms\n",
            "iter 180: loss 2.4378, time 96.24ms\n",
            "iter 190: loss 2.4072, time 95.79ms\n",
            "step 200: train loss 2.4214, val loss 2.4336\n",
            "iter 200: loss 2.4672, time 1557.22ms\n",
            "iter 210: loss 2.4358, time 98.41ms\n",
            "iter 220: loss 2.3892, time 98.40ms\n",
            "iter 230: loss 2.3826, time 98.29ms\n",
            "iter 240: loss 2.4222, time 97.83ms\n",
            "iter 250: loss 2.4291, time 96.96ms\n",
            "iter 260: loss 2.4200, time 97.57ms\n",
            "iter 270: loss 2.3870, time 97.13ms\n",
            "iter 280: loss 2.3742, time 96.71ms\n",
            "iter 290: loss 2.3679, time 98.58ms\n",
            "iter 300: loss 2.3828, time 98.32ms\n",
            "iter 310: loss 2.3923, time 97.81ms\n",
            "iter 320: loss 2.4388, time 99.82ms\n",
            "iter 330: loss 2.3659, time 98.88ms\n",
            "iter 340: loss 2.4077, time 99.11ms\n",
            "iter 350: loss 2.4447, time 97.15ms\n",
            "iter 360: loss 2.3885, time 97.73ms\n",
            "iter 370: loss 2.4363, time 99.85ms\n",
            "iter 380: loss 2.3968, time 99.55ms\n",
            "iter 390: loss 2.3751, time 100.53ms\n",
            "step 400: train loss 2.3554, val loss 2.3642\n",
            "iter 400: loss 2.3533, time 1599.85ms\n",
            "iter 410: loss 2.3919, time 100.09ms\n",
            "iter 420: loss 2.3321, time 100.67ms\n",
            "iter 430: loss 2.3360, time 101.21ms\n",
            "iter 440: loss 2.3715, time 101.99ms\n",
            "iter 450: loss 2.3642, time 100.86ms\n",
            "iter 460: loss 2.3558, time 101.62ms\n",
            "iter 470: loss 2.3258, time 101.45ms\n",
            "iter 480: loss 2.3130, time 101.23ms\n",
            "iter 490: loss 2.2868, time 101.95ms\n",
            "iter 500: loss 2.3099, time 101.78ms\n",
            "iter 510: loss 2.2623, time 101.56ms\n",
            "iter 520: loss 2.2629, time 102.85ms\n",
            "iter 530: loss 2.3522, time 104.60ms\n",
            "iter 540: loss 2.2688, time 108.53ms\n",
            "iter 550: loss 2.2349, time 102.38ms\n",
            "iter 560: loss 2.2034, time 103.34ms\n",
            "iter 570: loss 2.2245, time 103.00ms\n",
            "iter 580: loss 2.2005, time 103.48ms\n",
            "iter 590: loss 2.1673, time 103.85ms\n",
            "step 600: train loss 2.1500, val loss 2.1549\n",
            "iter 600: loss 2.1424, time 1652.43ms\n",
            "iter 610: loss 2.2359, time 103.24ms\n",
            "iter 620: loss 2.1011, time 104.05ms\n",
            "iter 630: loss 2.1637, time 103.42ms\n",
            "iter 640: loss 2.1649, time 104.05ms\n",
            "iter 650: loss 2.1001, time 105.61ms\n",
            "iter 660: loss 2.1476, time 105.17ms\n",
            "iter 670: loss 2.1357, time 105.16ms\n",
            "iter 680: loss 2.0612, time 105.91ms\n",
            "iter 690: loss 2.1021, time 105.65ms\n",
            "iter 700: loss 2.0860, time 104.99ms\n",
            "iter 710: loss 2.0933, time 105.10ms\n",
            "iter 720: loss 2.0998, time 105.20ms\n",
            "iter 730: loss 2.1418, time 105.51ms\n",
            "iter 740: loss 2.0818, time 106.03ms\n",
            "iter 750: loss 2.0680, time 105.39ms\n",
            "iter 760: loss 2.0906, time 105.71ms\n",
            "iter 770: loss 2.0420, time 106.60ms\n",
            "iter 780: loss 2.0157, time 106.11ms\n",
            "iter 790: loss 2.1044, time 106.61ms\n",
            "step 800: train loss 1.9962, val loss 1.9855\n",
            "iter 800: loss 2.0368, time 1714.17ms\n",
            "iter 810: loss 2.0751, time 107.90ms\n",
            "iter 820: loss 1.9859, time 107.65ms\n",
            "iter 830: loss 1.9984, time 108.81ms\n",
            "iter 840: loss 1.9808, time 108.90ms\n",
            "iter 850: loss 1.9713, time 108.00ms\n",
            "iter 860: loss 2.0403, time 109.37ms\n",
            "iter 870: loss 2.0445, time 108.28ms\n",
            "iter 880: loss 1.9615, time 107.68ms\n",
            "iter 890: loss 1.9730, time 108.94ms\n",
            "iter 900: loss 2.0324, time 108.17ms\n",
            "iter 910: loss 1.9906, time 108.42ms\n",
            "iter 920: loss 2.0127, time 110.36ms\n",
            "iter 930: loss 1.8937, time 109.40ms\n",
            "iter 940: loss 2.0030, time 109.09ms\n",
            "iter 950: loss 1.9113, time 109.34ms\n",
            "iter 960: loss 2.0181, time 109.54ms\n",
            "iter 970: loss 1.9930, time 109.30ms\n",
            "iter 980: loss 1.9500, time 109.28ms\n",
            "iter 990: loss 1.9596, time 110.07ms\n",
            "step 1000: train loss 1.8806, val loss 1.8674\n",
            "iter 1000: loss 1.8677, time 1739.46ms\n",
            "iter 1010: loss 1.9406, time 108.43ms\n",
            "iter 1020: loss 1.9302, time 109.35ms\n",
            "iter 1030: loss 1.8889, time 108.78ms\n",
            "iter 1040: loss 1.9969, time 109.07ms\n",
            "iter 1050: loss 1.9156, time 108.76ms\n",
            "iter 1060: loss 1.8717, time 106.24ms\n",
            "iter 1070: loss 1.8694, time 108.13ms\n",
            "iter 1080: loss 1.8474, time 108.23ms\n",
            "iter 1090: loss 1.8934, time 107.26ms\n",
            "iter 1100: loss 1.9358, time 108.17ms\n",
            "iter 1110: loss 1.8651, time 108.36ms\n",
            "iter 1120: loss 1.8834, time 107.00ms\n",
            "iter 1130: loss 1.9281, time 107.66ms\n",
            "iter 1140: loss 1.8895, time 107.18ms\n",
            "iter 1150: loss 1.9350, time 107.52ms\n",
            "iter 1160: loss 1.8300, time 107.12ms\n",
            "iter 1170: loss 1.9251, time 106.86ms\n",
            "iter 1180: loss 1.8815, time 106.81ms\n",
            "iter 1190: loss 1.8790, time 106.72ms\n",
            "step 1200: train loss 1.7631, val loss 1.7843\n",
            "iter 1200: loss 1.8810, time 1704.58ms\n",
            "iter 1210: loss 1.8233, time 107.27ms\n",
            "iter 1220: loss 1.8738, time 107.12ms\n",
            "iter 1230: loss 1.8422, time 107.86ms\n",
            "iter 1240: loss 1.8293, time 106.42ms\n",
            "iter 1250: loss 1.8601, time 105.73ms\n",
            "iter 1260: loss 1.7968, time 105.44ms\n",
            "iter 1270: loss 1.8075, time 106.48ms\n",
            "iter 1280: loss 1.8693, time 106.39ms\n",
            "iter 1290: loss 1.8884, time 106.38ms\n",
            "iter 1300: loss 1.8208, time 105.20ms\n",
            "iter 1310: loss 1.8101, time 107.00ms\n",
            "iter 1320: loss 1.8248, time 105.65ms\n",
            "iter 1330: loss 1.8565, time 106.76ms\n",
            "iter 1340: loss 1.8212, time 106.21ms\n",
            "iter 1350: loss 1.8038, time 107.25ms\n",
            "iter 1360: loss 1.8306, time 105.92ms\n",
            "iter 1370: loss 1.8009, time 105.76ms\n",
            "iter 1380: loss 1.8527, time 106.37ms\n",
            "iter 1390: loss 1.8573, time 106.60ms\n",
            "step 1400: train loss 1.7032, val loss 1.7365\n",
            "iter 1400: loss 1.8276, time 1696.79ms\n",
            "iter 1410: loss 1.8106, time 105.77ms\n",
            "iter 1420: loss 1.7947, time 106.29ms\n",
            "iter 1430: loss 1.8072, time 106.25ms\n",
            "iter 1440: loss 1.7550, time 106.97ms\n",
            "iter 1450: loss 1.7791, time 107.49ms\n",
            "iter 1460: loss 1.7368, time 106.87ms\n",
            "iter 1470: loss 1.7457, time 106.48ms\n",
            "iter 1480: loss 1.7826, time 106.90ms\n",
            "iter 1490: loss 1.7071, time 107.24ms\n",
            "iter 1500: loss 1.8050, time 106.29ms\n",
            "iter 1510: loss 1.7240, time 107.01ms\n",
            "iter 1520: loss 1.7725, time 107.35ms\n",
            "iter 1530: loss 1.7646, time 107.28ms\n",
            "iter 1540: loss 1.7694, time 107.87ms\n",
            "iter 1550: loss 1.7935, time 107.58ms\n",
            "iter 1560: loss 1.8362, time 106.93ms\n",
            "iter 1570: loss 1.7847, time 106.57ms\n",
            "iter 1580: loss 1.6517, time 107.15ms\n",
            "iter 1590: loss 1.7565, time 106.80ms\n",
            "step 1600: train loss 1.6508, val loss 1.6827\n",
            "iter 1600: loss 1.6819, time 1712.08ms\n",
            "iter 1610: loss 1.7546, time 106.79ms\n",
            "iter 1620: loss 1.7595, time 106.61ms\n",
            "iter 1630: loss 1.7304, time 107.21ms\n",
            "iter 1640: loss 1.7734, time 107.77ms\n",
            "iter 1650: loss 1.7065, time 107.95ms\n",
            "iter 1660: loss 1.7408, time 107.59ms\n",
            "iter 1670: loss 1.7017, time 106.87ms\n",
            "iter 1680: loss 1.7234, time 109.52ms\n",
            "iter 1690: loss 1.7416, time 108.24ms\n",
            "iter 1700: loss 1.7397, time 107.01ms\n",
            "iter 1710: loss 1.6858, time 107.98ms\n",
            "iter 1720: loss 1.6996, time 107.80ms\n",
            "iter 1730: loss 1.7121, time 106.70ms\n",
            "iter 1740: loss 1.7050, time 107.55ms\n",
            "iter 1750: loss 1.7062, time 106.88ms\n",
            "iter 1760: loss 1.7331, time 107.25ms\n",
            "iter 1770: loss 1.6757, time 107.91ms\n",
            "iter 1780: loss 1.7343, time 107.86ms\n",
            "iter 1790: loss 1.6846, time 108.79ms\n",
            "step 1800: train loss 1.6219, val loss 1.6453\n",
            "iter 1800: loss 1.6713, time 1721.16ms\n",
            "iter 1810: loss 1.6556, time 107.88ms\n",
            "iter 1820: loss 1.7015, time 108.47ms\n",
            "iter 1830: loss 1.6830, time 106.55ms\n",
            "iter 1840: loss 1.7006, time 107.20ms\n",
            "iter 1850: loss 1.6943, time 108.75ms\n",
            "iter 1860: loss 1.6691, time 107.32ms\n",
            "iter 1870: loss 1.6684, time 107.89ms\n",
            "iter 1880: loss 1.7073, time 106.69ms\n",
            "iter 1890: loss 1.7304, time 107.31ms\n",
            "iter 1900: loss 1.6980, time 107.12ms\n",
            "iter 1910: loss 1.7240, time 107.56ms\n",
            "iter 1920: loss 1.6677, time 107.21ms\n",
            "iter 1930: loss 1.7444, time 106.43ms\n",
            "iter 1940: loss 1.6930, time 106.44ms\n",
            "iter 1950: loss 1.6107, time 107.01ms\n",
            "iter 1960: loss 1.6520, time 107.59ms\n",
            "iter 1970: loss 1.6405, time 107.97ms\n",
            "iter 1980: loss 1.6990, time 106.88ms\n",
            "iter 1990: loss 1.6325, time 107.15ms\n",
            "step 2000: train loss 1.5954, val loss 1.6167\n",
            "iter 2000: loss 1.6118, time 1730.78ms\n",
            "Training finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_static_moas.py\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import tiktoken\n",
        "from moht_gpt import GPT, GPTConfig\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "out_dir = 'out'\n",
        "eval_interval = 200\n",
        "log_interval = 10\n",
        "eval_iters = 20\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume'\n",
        "# data\n",
        "dataset = 'wikitext-2'\n",
        "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 256 # context of up to 256 tokens\n",
        "# model\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "attention_type = 'static_moas' # 'baseline', 'static_moas', 'moas'\n",
        "load_balance_weight = 0.01 # weight for load balancing loss (only for 'moas')\n",
        "# adamw optimizer\n",
        "learning_rate = 1e-3 # max learning rate\n",
        "max_iters = 2000 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 100 # how many steps to warm up for\n",
        "lr_decay_iters = 2000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 1e-4 # minimum learning rate, should be ~= learning_rate/10\n",
        "# system\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Data Loading\n",
        "# -----------------------------------------------------------------------------\n",
        "def get_data():\n",
        "    # simple data loader for wikitext-2\n",
        "    # we will download it if it doesn't exist\n",
        "    data_dir = os.path.join('data', dataset)\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    input_file_path = os.path.join(data_dir, 'input.txt')\n",
        "    if not os.path.exists(input_file_path):\n",
        "        import requests\n",
        "        print(\"Downloading WikiText-2...\")\n",
        "        data_url = 'https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt'\n",
        "        try:\n",
        "            with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(requests.get(data_url).text)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download data: {e}\")\n",
        "            print(\"Creating dummy data instead.\")\n",
        "            with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"Hello world \" * 10000)\n",
        "\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    # tokenize (character level)\n",
        "    chars = sorted(list(set(data)))\n",
        "    vocab_size = len(chars)\n",
        "    print(f\"Vocab size: {vocab_size}\")\n",
        "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "    itos = { i:ch for i,ch in enumerate(chars) }\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "    train_ids = encode(data)\n",
        "    n = len(train_ids)\n",
        "    train_data = np.array(train_ids[:int(n*0.9)], dtype=np.uint16)\n",
        "    val_data = np.array(train_ids[int(n*0.9):], dtype=np.uint16)\n",
        "    return train_data, val_data, vocab_size\n",
        "\n",
        "train_data, val_data, vocab_size = get_data()\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Training Setup\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "# config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = contextlib.nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=vocab_size, dropout=dropout, attention_type=attention_type)\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "model.to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    # TODO: implement resume\n",
        "    pass\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Training Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "\n",
        "print(f\"Training on {device}...\")\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(local_iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if local_iter_num % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {local_iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if always_save_checkpoint:\n",
        "            if local_iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': local_iter_num,\n",
        "                    'best_val_loss': losses['val'],\n",
        "                    'config': config_keys,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "\n",
        "    if local_iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "\n",
        "            # Add load balancing loss for MoAS\n",
        "            if attention_type == 'moas':\n",
        "                lb_loss = model.get_load_balancing_loss(X)\n",
        "                loss = loss + load_balance_weight * lb_loss\n",
        "\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        # scaler.scale(loss).backward()\n",
        "        loss.backward()\n",
        "\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    optimizer.step()\n",
        "    # scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if local_iter_num % log_interval == 0:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        print(f\"iter {local_iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
        "\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if local_iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "print(\"Training finished!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvoeDt6AK0pF",
        "outputId": "2b5f4107-42ed-4003-ad0e-1c9d78290643"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_static_moas.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_static_moas.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9mc1ZQBMQIm",
        "outputId": "c4bf07e7-91ca-40b9-f00f-b7f436fb897a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 283\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "number of parameters: 15.15M\n",
            "num decayed parameter tensors: 74 with 15,247,488 parameters\n",
            "num non-decayed parameter tensors: 13 with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "Training on cuda...\n",
            "step 0: train loss 5.7434, val loss 5.7429\n",
            "iter 0: loss 5.7337, time 2952.34ms\n",
            "iter 10: loss 3.9412, time 180.81ms\n",
            "iter 20: loss 3.2909, time 181.95ms\n",
            "iter 30: loss 2.8385, time 181.82ms\n",
            "iter 40: loss 2.6259, time 182.42ms\n",
            "iter 50: loss 2.5571, time 182.66ms\n",
            "iter 60: loss 2.5139, time 184.81ms\n",
            "iter 70: loss 2.5352, time 185.40ms\n",
            "iter 80: loss 2.4910, time 184.85ms\n",
            "iter 90: loss 2.4766, time 187.46ms\n",
            "iter 100: loss 2.4954, time 185.71ms\n",
            "iter 110: loss 2.5384, time 189.98ms\n",
            "iter 120: loss 2.4409, time 190.31ms\n",
            "iter 130: loss 2.4294, time 188.19ms\n",
            "iter 140: loss 2.4617, time 188.84ms\n",
            "iter 150: loss 2.4251, time 188.59ms\n",
            "iter 160: loss 2.4296, time 193.15ms\n",
            "iter 170: loss 2.4381, time 191.08ms\n",
            "iter 180: loss 2.4169, time 191.86ms\n",
            "iter 190: loss 2.5015, time 193.51ms\n",
            "step 200: train loss 2.4102, val loss 2.4173\n",
            "iter 200: loss 2.3885, time 2890.42ms\n",
            "iter 210: loss 2.3857, time 197.05ms\n",
            "iter 220: loss 2.4486, time 195.68ms\n",
            "iter 230: loss 2.4119, time 196.65ms\n",
            "iter 240: loss 2.4447, time 196.66ms\n",
            "iter 250: loss 2.4119, time 195.43ms\n",
            "iter 260: loss 2.3521, time 197.91ms\n",
            "iter 270: loss 2.4677, time 199.40ms\n",
            "iter 280: loss 2.4018, time 197.62ms\n",
            "iter 290: loss 2.3904, time 200.32ms\n",
            "iter 300: loss 2.3964, time 201.09ms\n",
            "iter 310: loss 2.3921, time 199.31ms\n",
            "iter 320: loss 2.3841, time 201.01ms\n",
            "iter 330: loss 2.3997, time 204.29ms\n",
            "iter 340: loss 2.3840, time 203.49ms\n",
            "iter 350: loss 2.3614, time 200.15ms\n",
            "iter 360: loss 2.3393, time 197.35ms\n",
            "iter 370: loss 2.3749, time 178.70ms\n",
            "iter 380: loss 2.3607, time 221.39ms\n",
            "iter 390: loss 2.3607, time 188.54ms\n",
            "step 400: train loss 2.3266, val loss 2.3337\n",
            "iter 400: loss 2.3642, time 2931.97ms\n",
            "iter 410: loss 2.3831, time 198.23ms\n",
            "iter 420: loss 2.3292, time 194.36ms\n",
            "iter 430: loss 2.3256, time 195.55ms\n",
            "iter 440: loss 2.3105, time 195.60ms\n",
            "iter 450: loss 2.3435, time 197.56ms\n",
            "iter 460: loss 2.3141, time 194.04ms\n",
            "iter 470: loss 2.2693, time 196.85ms\n",
            "iter 480: loss 2.2928, time 192.30ms\n",
            "iter 490: loss 2.2801, time 195.52ms\n",
            "iter 500: loss 2.2777, time 193.30ms\n",
            "iter 510: loss 2.1882, time 196.81ms\n",
            "iter 520: loss 2.2236, time 192.27ms\n",
            "iter 530: loss 2.1898, time 193.16ms\n",
            "iter 540: loss 2.1733, time 194.88ms\n",
            "iter 550: loss 2.2259, time 193.95ms\n",
            "iter 560: loss 2.1578, time 194.94ms\n",
            "iter 570: loss 2.1120, time 197.53ms\n",
            "iter 580: loss 2.1528, time 196.52ms\n",
            "iter 590: loss 2.1851, time 195.95ms\n",
            "step 600: train loss 2.0714, val loss 2.0848\n",
            "iter 600: loss 2.0800, time 2903.17ms\n",
            "iter 610: loss 2.1148, time 193.26ms\n",
            "iter 620: loss 2.1098, time 197.14ms\n",
            "iter 630: loss 2.1613, time 193.24ms\n",
            "iter 640: loss 2.0902, time 193.06ms\n",
            "iter 650: loss 2.0736, time 195.27ms\n",
            "iter 660: loss 2.0569, time 195.16ms\n",
            "iter 670: loss 2.0963, time 197.06ms\n",
            "iter 680: loss 2.0158, time 196.00ms\n",
            "iter 690: loss 2.0448, time 196.93ms\n",
            "iter 700: loss 1.9864, time 198.53ms\n",
            "iter 710: loss 2.0128, time 198.55ms\n",
            "iter 720: loss 2.0951, time 196.10ms\n",
            "iter 730: loss 1.9912, time 197.73ms\n",
            "iter 740: loss 2.0081, time 195.82ms\n",
            "iter 750: loss 1.9360, time 197.69ms\n",
            "iter 760: loss 2.0054, time 196.14ms\n",
            "iter 770: loss 2.0646, time 193.35ms\n",
            "iter 780: loss 1.9600, time 197.68ms\n",
            "iter 790: loss 1.9627, time 197.58ms\n",
            "step 800: train loss 1.9091, val loss 1.9247\n",
            "iter 800: loss 2.0080, time 2926.13ms\n",
            "iter 810: loss 1.9230, time 197.33ms\n",
            "iter 820: loss 1.9558, time 198.59ms\n",
            "iter 830: loss 1.9419, time 194.18ms\n",
            "iter 840: loss 1.9839, time 192.21ms\n",
            "iter 850: loss 1.8939, time 195.38ms\n",
            "iter 860: loss 1.9469, time 193.79ms\n",
            "iter 870: loss 1.9663, time 196.57ms\n",
            "iter 880: loss 1.8846, time 197.07ms\n",
            "iter 890: loss 1.8166, time 193.68ms\n",
            "iter 900: loss 1.8909, time 196.43ms\n",
            "iter 910: loss 1.9164, time 196.09ms\n",
            "iter 920: loss 1.8693, time 192.01ms\n",
            "iter 930: loss 1.8854, time 197.71ms\n",
            "iter 940: loss 1.9057, time 192.62ms\n",
            "iter 950: loss 1.9158, time 196.80ms\n",
            "iter 960: loss 1.8963, time 193.47ms\n",
            "iter 970: loss 1.9551, time 195.82ms\n",
            "iter 980: loss 1.8443, time 197.40ms\n",
            "iter 990: loss 1.8779, time 192.64ms\n",
            "step 1000: train loss 1.8172, val loss 1.8244\n",
            "iter 1000: loss 1.8752, time 2917.34ms\n",
            "iter 1010: loss 1.7900, time 196.22ms\n",
            "iter 1020: loss 1.8503, time 193.90ms\n",
            "iter 1030: loss 1.7947, time 194.11ms\n",
            "iter 1040: loss 1.8666, time 193.90ms\n",
            "iter 1050: loss 1.8446, time 195.71ms\n",
            "iter 1060: loss 1.8034, time 195.94ms\n",
            "iter 1070: loss 1.8821, time 197.69ms\n",
            "iter 1080: loss 1.8019, time 198.27ms\n",
            "iter 1090: loss 1.8462, time 196.30ms\n",
            "iter 1100: loss 1.7725, time 195.08ms\n",
            "iter 1110: loss 1.8145, time 193.97ms\n",
            "iter 1120: loss 1.8129, time 193.22ms\n",
            "iter 1130: loss 1.8520, time 192.82ms\n",
            "iter 1140: loss 1.8071, time 195.07ms\n",
            "iter 1150: loss 1.7730, time 197.63ms\n",
            "iter 1160: loss 1.7838, time 197.93ms\n",
            "iter 1170: loss 1.7733, time 198.26ms\n",
            "iter 1180: loss 1.8282, time 195.33ms\n",
            "iter 1190: loss 1.7570, time 195.04ms\n",
            "step 1200: train loss 1.7024, val loss 1.7405\n",
            "iter 1200: loss 1.7619, time 2928.76ms\n",
            "iter 1210: loss 1.7750, time 194.22ms\n",
            "iter 1220: loss 1.7368, time 196.30ms\n",
            "iter 1230: loss 1.7654, time 197.03ms\n",
            "iter 1240: loss 1.7735, time 194.27ms\n",
            "iter 1250: loss 1.7661, time 197.71ms\n",
            "iter 1260: loss 1.7627, time 197.83ms\n",
            "iter 1270: loss 1.8349, time 198.77ms\n",
            "iter 1280: loss 1.7156, time 198.49ms\n",
            "iter 1290: loss 1.7911, time 198.05ms\n",
            "iter 1300: loss 1.7241, time 199.56ms\n",
            "iter 1310: loss 1.7931, time 197.58ms\n",
            "iter 1320: loss 1.7784, time 197.21ms\n",
            "iter 1330: loss 1.7369, time 198.21ms\n",
            "iter 1340: loss 1.7444, time 196.31ms\n",
            "iter 1350: loss 1.7424, time 195.71ms\n",
            "iter 1360: loss 1.7590, time 197.65ms\n",
            "iter 1370: loss 1.7446, time 196.90ms\n",
            "iter 1380: loss 1.7374, time 196.17ms\n",
            "iter 1390: loss 1.7071, time 196.00ms\n",
            "step 1400: train loss 1.6468, val loss 1.6748\n",
            "iter 1400: loss 1.7288, time 2923.94ms\n",
            "iter 1410: loss 1.6466, time 195.23ms\n",
            "iter 1420: loss 1.6233, time 197.95ms\n",
            "iter 1430: loss 1.6535, time 195.98ms\n",
            "iter 1440: loss 1.7252, time 196.20ms\n",
            "iter 1450: loss 1.6691, time 197.57ms\n",
            "iter 1460: loss 1.6615, time 197.80ms\n",
            "iter 1470: loss 1.6366, time 197.35ms\n",
            "iter 1480: loss 1.7248, time 197.77ms\n",
            "iter 1490: loss 1.7272, time 197.85ms\n",
            "iter 1500: loss 1.7101, time 197.72ms\n",
            "iter 1510: loss 1.6819, time 197.46ms\n",
            "iter 1520: loss 1.6967, time 196.30ms\n",
            "iter 1530: loss 1.6718, time 194.77ms\n",
            "iter 1540: loss 1.6832, time 194.34ms\n",
            "iter 1550: loss 1.6728, time 196.78ms\n",
            "iter 1560: loss 1.6679, time 199.04ms\n",
            "iter 1570: loss 1.6704, time 197.42ms\n",
            "iter 1580: loss 1.6546, time 192.80ms\n",
            "iter 1590: loss 1.6791, time 195.32ms\n",
            "step 1600: train loss 1.5989, val loss 1.6198\n",
            "iter 1600: loss 1.6107, time 2920.53ms\n",
            "iter 1610: loss 1.6961, time 194.21ms\n",
            "iter 1620: loss 1.7178, time 193.59ms\n",
            "iter 1630: loss 1.6634, time 196.73ms\n",
            "iter 1640: loss 1.6267, time 197.27ms\n",
            "iter 1650: loss 1.6774, time 197.44ms\n",
            "iter 1660: loss 1.6642, time 196.62ms\n",
            "iter 1670: loss 1.6947, time 194.32ms\n",
            "iter 1680: loss 1.6046, time 194.83ms\n",
            "iter 1690: loss 1.5952, time 194.57ms\n",
            "iter 1700: loss 1.6420, time 193.94ms\n",
            "iter 1710: loss 1.6892, time 196.71ms\n",
            "iter 1720: loss 1.6889, time 196.28ms\n",
            "iter 1730: loss 1.6935, time 193.70ms\n",
            "iter 1740: loss 1.6875, time 194.39ms\n",
            "iter 1750: loss 1.6574, time 192.81ms\n",
            "iter 1760: loss 1.6680, time 197.31ms\n",
            "iter 1770: loss 1.7327, time 199.12ms\n",
            "iter 1780: loss 1.6618, time 197.87ms\n",
            "iter 1790: loss 1.7054, time 198.10ms\n",
            "step 1800: train loss 1.5566, val loss 1.6006\n",
            "iter 1800: loss 1.6041, time 2928.84ms\n",
            "iter 1810: loss 1.5993, time 195.90ms\n",
            "iter 1820: loss 1.6877, time 197.58ms\n",
            "iter 1830: loss 1.5906, time 198.37ms\n",
            "iter 1840: loss 1.6128, time 198.43ms\n",
            "iter 1850: loss 1.6699, time 196.49ms\n",
            "iter 1860: loss 1.6832, time 193.25ms\n",
            "iter 1870: loss 1.6128, time 193.20ms\n",
            "iter 1880: loss 1.6139, time 193.26ms\n",
            "iter 1890: loss 1.6140, time 195.75ms\n",
            "iter 1900: loss 1.6284, time 197.35ms\n",
            "iter 1910: loss 1.6204, time 194.09ms\n",
            "iter 1920: loss 1.6174, time 194.74ms\n",
            "iter 1930: loss 1.6227, time 195.70ms\n",
            "iter 1940: loss 1.5930, time 191.94ms\n",
            "iter 1950: loss 1.6680, time 195.96ms\n",
            "iter 1960: loss 1.6769, time 197.40ms\n",
            "iter 1970: loss 1.6335, time 196.11ms\n",
            "iter 1980: loss 1.6149, time 196.78ms\n",
            "iter 1990: loss 1.6115, time 194.31ms\n",
            "step 2000: train loss 1.5361, val loss 1.5802\n",
            "iter 2000: loss 1.6480, time 2924.78ms\n",
            "Training finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_moas.py\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import tiktoken\n",
        "from moht_gpt import GPT, GPTConfig\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "out_dir = 'out'\n",
        "eval_interval = 200\n",
        "log_interval = 10\n",
        "eval_iters = 20\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume'\n",
        "# data\n",
        "dataset = 'wikitext-2'\n",
        "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 256 # context of up to 256 tokens\n",
        "# model\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "attention_type = 'moas' # 'baseline', 'static_moas', 'moas'\n",
        "load_balance_weight = 0.01 # weight for load balancing loss (only for 'moas')\n",
        "# adamw optimizer\n",
        "learning_rate = 1e-3 # max learning rate\n",
        "max_iters = 2000 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 100 # how many steps to warm up for\n",
        "lr_decay_iters = 2000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 1e-4 # minimum learning rate, should be ~= learning_rate/10\n",
        "# system\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Data Loading\n",
        "# -----------------------------------------------------------------------------\n",
        "def get_data():\n",
        "    # simple data loader for wikitext-2\n",
        "    # we will download it if it doesn't exist\n",
        "    data_dir = os.path.join('data', dataset)\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    input_file_path = os.path.join(data_dir, 'input.txt')\n",
        "    if not os.path.exists(input_file_path):\n",
        "        import requests\n",
        "        print(\"Downloading WikiText-2...\")\n",
        "        data_url = 'https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt'\n",
        "        try:\n",
        "            with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(requests.get(data_url).text)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download data: {e}\")\n",
        "            print(\"Creating dummy data instead.\")\n",
        "            with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"Hello world \" * 10000)\n",
        "\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    # tokenize (character level)\n",
        "    chars = sorted(list(set(data)))\n",
        "    vocab_size = len(chars)\n",
        "    print(f\"Vocab size: {vocab_size}\")\n",
        "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "    itos = { i:ch for i,ch in enumerate(chars) }\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "    train_ids = encode(data)\n",
        "    n = len(train_ids)\n",
        "    train_data = np.array(train_ids[:int(n*0.9)], dtype=np.uint16)\n",
        "    val_data = np.array(train_ids[int(n*0.9):], dtype=np.uint16)\n",
        "    return train_data, val_data, vocab_size\n",
        "\n",
        "train_data, val_data, vocab_size = get_data()\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Training Setup\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "# config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = contextlib.nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=vocab_size, dropout=dropout, attention_type=attention_type)\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "model.to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    # TODO: implement resume\n",
        "    pass\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Training Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "\n",
        "print(f\"Training on {device}...\")\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(local_iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if local_iter_num % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {local_iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if always_save_checkpoint:\n",
        "            if local_iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': local_iter_num,\n",
        "                    'best_val_loss': losses['val'],\n",
        "                    'config': config_keys,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "\n",
        "    if local_iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "\n",
        "            # Add load balancing loss for MoAS\n",
        "            if attention_type == 'moas':\n",
        "                lb_loss = model.get_load_balancing_loss(X)\n",
        "                loss = loss + load_balance_weight * lb_loss\n",
        "\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        # scaler.scale(loss).backward()\n",
        "        loss.backward()\n",
        "\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    optimizer.step()\n",
        "    # scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if local_iter_num % log_interval == 0:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        print(f\"iter {local_iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
        "\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if local_iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "print(\"Training finished!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuKp3_LrMT92",
        "outputId": "d39b0654-d4b1-4648-a115-2c932bd07028"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_moas.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_moas.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_cqGatiOMC2",
        "outputId": "78f18425-ffa0-4d71-dcea-952231cdcd86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 283\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "number of parameters: 15.38M\n",
            "num decayed parameter tensors: 86 with 15,470,400 parameters\n",
            "num non-decayed parameter tensors: 13 with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "Training on cuda...\n",
            "step 0: train loss 5.6328, val loss 5.6301\n",
            "iter 0: loss 5.6384, time 3363.34ms\n",
            "iter 10: loss 3.8911, time 365.71ms\n",
            "iter 20: loss 3.2655, time 369.08ms\n",
            "iter 30: loss 2.8054, time 372.64ms\n",
            "iter 40: loss 2.5900, time 375.00ms\n",
            "iter 50: loss 2.5962, time 374.98ms\n",
            "iter 60: loss 2.5543, time 377.77ms\n",
            "iter 70: loss 2.5553, time 382.30ms\n",
            "iter 80: loss 2.5149, time 383.75ms\n",
            "iter 90: loss 2.4737, time 386.36ms\n",
            "iter 100: loss 2.4753, time 390.71ms\n",
            "iter 110: loss 2.4623, time 393.08ms\n",
            "iter 120: loss 2.4999, time 398.33ms\n",
            "iter 130: loss 2.4667, time 393.36ms\n",
            "iter 140: loss 2.4477, time 399.29ms\n",
            "iter 150: loss 2.4524, time 392.15ms\n",
            "iter 160: loss 2.3879, time 389.80ms\n",
            "iter 170: loss 2.4681, time 408.02ms\n",
            "iter 180: loss 2.4510, time 387.26ms\n",
            "iter 190: loss 2.4377, time 389.79ms\n",
            "step 200: train loss 2.4189, val loss 2.4319\n",
            "iter 200: loss 2.3753, time 3193.27ms\n",
            "iter 210: loss 2.4449, time 383.44ms\n",
            "iter 220: loss 2.4117, time 383.00ms\n",
            "iter 230: loss 2.4020, time 382.90ms\n",
            "iter 240: loss 2.4636, time 382.96ms\n",
            "iter 250: loss 2.3782, time 387.31ms\n",
            "iter 260: loss 2.4804, time 382.76ms\n",
            "iter 270: loss 2.4145, time 382.29ms\n",
            "iter 280: loss 2.4051, time 383.29ms\n",
            "iter 290: loss 2.3911, time 385.20ms\n",
            "iter 300: loss 2.4350, time 385.11ms\n",
            "iter 310: loss 2.4440, time 387.38ms\n",
            "iter 320: loss 2.4087, time 386.71ms\n",
            "iter 330: loss 2.3966, time 387.54ms\n",
            "iter 340: loss 2.4016, time 388.02ms\n",
            "iter 350: loss 2.3214, time 387.70ms\n",
            "iter 360: loss 2.3877, time 386.70ms\n",
            "iter 370: loss 2.4283, time 391.98ms\n",
            "iter 380: loss 2.3590, time 425.51ms\n",
            "iter 390: loss 2.4120, time 389.89ms\n",
            "step 400: train loss 2.3569, val loss 2.3654\n",
            "iter 400: loss 2.3363, time 3225.74ms\n",
            "iter 410: loss 2.3909, time 395.76ms\n",
            "iter 420: loss 2.3596, time 383.16ms\n",
            "iter 430: loss 2.3595, time 386.17ms\n",
            "iter 440: loss 2.3433, time 383.65ms\n",
            "iter 450: loss 2.3437, time 382.53ms\n",
            "iter 460: loss 2.3379, time 382.82ms\n",
            "iter 470: loss 2.3377, time 382.23ms\n",
            "iter 480: loss 2.3305, time 386.63ms\n",
            "iter 490: loss 2.3792, time 384.83ms\n",
            "iter 500: loss 2.3544, time 384.42ms\n",
            "iter 510: loss 2.3527, time 385.28ms\n",
            "iter 520: loss 2.3638, time 387.43ms\n",
            "iter 530: loss 2.3015, time 386.82ms\n",
            "iter 540: loss 2.2677, time 387.23ms\n",
            "iter 550: loss 2.2873, time 385.18ms\n",
            "iter 560: loss 2.2792, time 388.48ms\n",
            "iter 570: loss 2.3129, time 387.76ms\n",
            "iter 580: loss 2.2575, time 386.31ms\n",
            "iter 590: loss 2.2007, time 387.55ms\n",
            "step 600: train loss 2.1873, val loss 2.1894\n",
            "iter 600: loss 2.2232, time 3224.17ms\n",
            "iter 610: loss 2.2406, time 386.04ms\n",
            "iter 620: loss 2.2343, time 389.27ms\n",
            "iter 630: loss 2.2227, time 410.46ms\n",
            "iter 640: loss 2.1977, time 390.52ms\n",
            "iter 650: loss 2.1840, time 390.44ms\n",
            "iter 660: loss 2.2415, time 386.26ms\n",
            "iter 670: loss 2.1776, time 383.80ms\n",
            "iter 680: loss 2.1640, time 386.68ms\n",
            "iter 690: loss 2.1887, time 391.10ms\n",
            "iter 700: loss 2.1608, time 390.78ms\n",
            "iter 710: loss 2.1047, time 374.91ms\n",
            "iter 720: loss 2.1214, time 393.15ms\n",
            "iter 730: loss 2.2042, time 387.95ms\n",
            "iter 740: loss 2.0874, time 387.06ms\n",
            "iter 750: loss 2.0448, time 385.23ms\n",
            "iter 760: loss 2.1450, time 386.15ms\n",
            "iter 770: loss 2.0779, time 387.14ms\n",
            "iter 780: loss 2.1702, time 389.20ms\n",
            "iter 790: loss 2.0834, time 385.53ms\n",
            "step 800: train loss 2.0137, val loss 2.0276\n",
            "iter 800: loss 2.0836, time 3234.99ms\n",
            "iter 810: loss 2.0439, time 390.74ms\n",
            "iter 820: loss 2.1220, time 387.84ms\n",
            "iter 830: loss 2.0691, time 387.07ms\n",
            "iter 840: loss 2.0883, time 385.69ms\n",
            "iter 850: loss 2.0581, time 385.85ms\n",
            "iter 860: loss 2.0006, time 384.77ms\n",
            "iter 870: loss 2.0133, time 386.21ms\n",
            "iter 880: loss 1.9911, time 387.88ms\n",
            "iter 890: loss 2.0708, time 384.67ms\n",
            "iter 900: loss 2.1067, time 387.48ms\n",
            "iter 910: loss 1.9877, time 389.15ms\n",
            "iter 920: loss 2.0236, time 384.92ms\n",
            "iter 930: loss 2.0209, time 385.79ms\n",
            "iter 940: loss 2.0322, time 388.17ms\n",
            "iter 950: loss 1.9627, time 385.93ms\n",
            "iter 960: loss 2.0074, time 390.81ms\n",
            "iter 970: loss 2.0351, time 385.37ms\n",
            "iter 980: loss 2.0468, time 386.40ms\n",
            "iter 990: loss 2.0069, time 391.56ms\n",
            "step 1000: train loss 1.8972, val loss 1.9250\n",
            "iter 1000: loss 1.9990, time 3226.50ms\n",
            "iter 1010: loss 1.9171, time 386.18ms\n",
            "iter 1020: loss 1.9903, time 385.58ms\n",
            "iter 1030: loss 2.0209, time 388.72ms\n",
            "iter 1040: loss 1.9862, time 385.04ms\n",
            "iter 1050: loss 1.9686, time 387.61ms\n",
            "iter 1060: loss 2.0359, time 386.10ms\n",
            "iter 1070: loss 1.9267, time 391.22ms\n",
            "iter 1080: loss 1.9459, time 385.41ms\n",
            "iter 1090: loss 1.9484, time 390.05ms\n",
            "iter 1100: loss 1.9280, time 384.26ms\n",
            "iter 1110: loss 1.9706, time 385.58ms\n",
            "iter 1120: loss 1.9010, time 392.36ms\n",
            "iter 1130: loss 1.8611, time 385.50ms\n",
            "iter 1140: loss 1.8727, time 386.42ms\n",
            "iter 1150: loss 1.9744, time 390.27ms\n",
            "iter 1160: loss 1.9457, time 388.26ms\n",
            "iter 1170: loss 1.8944, time 386.63ms\n",
            "iter 1180: loss 1.8754, time 386.34ms\n",
            "iter 1190: loss 1.8880, time 386.34ms\n",
            "step 1200: train loss 1.8191, val loss 1.8328\n",
            "iter 1200: loss 1.8076, time 3234.79ms\n",
            "iter 1210: loss 1.8485, time 390.31ms\n",
            "iter 1220: loss 1.8956, time 388.91ms\n",
            "iter 1230: loss 1.8969, time 392.29ms\n",
            "iter 1240: loss 1.8971, time 389.97ms\n",
            "iter 1250: loss 1.8871, time 386.37ms\n",
            "iter 1260: loss 1.8764, time 384.29ms\n",
            "iter 1270: loss 1.8630, time 393.94ms\n",
            "iter 1280: loss 1.8378, time 386.45ms\n",
            "iter 1290: loss 1.9073, time 387.86ms\n",
            "iter 1300: loss 1.8378, time 384.20ms\n",
            "iter 1310: loss 1.8391, time 384.84ms\n",
            "iter 1320: loss 1.8396, time 388.30ms\n",
            "iter 1330: loss 1.8937, time 385.37ms\n",
            "iter 1340: loss 1.8743, time 386.41ms\n",
            "iter 1350: loss 1.8752, time 384.85ms\n",
            "iter 1360: loss 1.8426, time 384.93ms\n",
            "iter 1370: loss 1.8027, time 388.44ms\n",
            "iter 1380: loss 1.8537, time 387.58ms\n",
            "iter 1390: loss 1.7994, time 389.96ms\n",
            "step 1400: train loss 1.7344, val loss 1.7527\n",
            "iter 1400: loss 1.8177, time 3229.72ms\n",
            "iter 1410: loss 1.9292, time 386.75ms\n",
            "iter 1420: loss 1.7619, time 389.31ms\n",
            "iter 1430: loss 1.8410, time 386.60ms\n",
            "iter 1440: loss 1.8196, time 385.57ms\n",
            "iter 1450: loss 1.7329, time 386.37ms\n",
            "iter 1460: loss 1.8506, time 386.70ms\n",
            "iter 1470: loss 1.7940, time 387.79ms\n",
            "iter 1480: loss 1.8410, time 388.85ms\n",
            "iter 1490: loss 1.8122, time 393.10ms\n",
            "iter 1500: loss 1.8047, time 383.28ms\n",
            "iter 1510: loss 1.8466, time 385.35ms\n",
            "iter 1520: loss 1.8771, time 390.54ms\n",
            "iter 1530: loss 1.8322, time 388.28ms\n",
            "iter 1540: loss 1.8400, time 386.28ms\n",
            "iter 1550: loss 1.7016, time 384.52ms\n",
            "iter 1560: loss 1.8275, time 388.77ms\n",
            "iter 1570: loss 1.8083, time 388.03ms\n",
            "iter 1580: loss 1.8333, time 386.57ms\n",
            "iter 1590: loss 1.7588, time 389.19ms\n",
            "step 1600: train loss 1.6872, val loss 1.7160\n",
            "iter 1600: loss 1.7465, time 3223.83ms\n",
            "iter 1610: loss 1.7852, time 385.86ms\n",
            "iter 1620: loss 1.7714, time 387.06ms\n",
            "iter 1630: loss 1.6966, time 390.48ms\n",
            "iter 1640: loss 1.7793, time 390.00ms\n",
            "iter 1650: loss 1.7890, time 387.99ms\n",
            "iter 1660: loss 1.7317, time 388.11ms\n",
            "iter 1670: loss 1.7571, time 386.36ms\n",
            "iter 1680: loss 1.7727, time 386.23ms\n",
            "iter 1690: loss 1.7477, time 391.74ms\n",
            "iter 1700: loss 1.7793, time 387.44ms\n",
            "iter 1710: loss 1.7491, time 391.44ms\n",
            "iter 1720: loss 1.7334, time 385.50ms\n",
            "iter 1730: loss 1.7220, time 385.93ms\n",
            "iter 1740: loss 1.7609, time 387.09ms\n",
            "iter 1750: loss 1.7285, time 385.66ms\n",
            "iter 1760: loss 1.7854, time 388.88ms\n",
            "iter 1770: loss 1.7675, time 386.50ms\n",
            "iter 1780: loss 1.7018, time 391.43ms\n",
            "iter 1790: loss 1.7454, time 385.52ms\n",
            "step 1800: train loss 1.6549, val loss 1.7004\n",
            "iter 1800: loss 1.7537, time 3230.39ms\n",
            "iter 1810: loss 1.7110, time 387.09ms\n",
            "iter 1820: loss 1.7836, time 387.27ms\n",
            "iter 1830: loss 1.7421, time 391.61ms\n",
            "iter 1840: loss 1.7170, time 385.25ms\n",
            "iter 1850: loss 1.7137, time 383.92ms\n",
            "iter 1860: loss 1.7456, time 386.43ms\n",
            "iter 1870: loss 1.7156, time 389.48ms\n",
            "iter 1880: loss 1.6539, time 388.05ms\n",
            "iter 1890: loss 1.7184, time 388.02ms\n",
            "iter 1900: loss 1.7749, time 386.47ms\n",
            "iter 1910: loss 1.7818, time 384.74ms\n",
            "iter 1920: loss 1.7221, time 390.92ms\n",
            "iter 1930: loss 1.7752, time 391.09ms\n",
            "iter 1940: loss 1.7173, time 383.82ms\n",
            "iter 1950: loss 1.6935, time 386.48ms\n",
            "iter 1960: loss 1.7783, time 393.18ms\n",
            "iter 1970: loss 1.8260, time 389.61ms\n",
            "iter 1980: loss 1.7046, time 385.76ms\n",
            "iter 1990: loss 1.8278, time 384.41ms\n",
            "step 2000: train loss 1.6248, val loss 1.6618\n",
            "iter 2000: loss 1.7344, time 3220.49ms\n",
            "Training finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile compare_moas.py\n",
        "\"\"\"\n",
        "Compare training runs for Baseline, Static MoAS, and Dynamic MoAS\n",
        "\"\"\"\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from moht_gpt import GPT, GPTConfig\n",
        "\n",
        "# Configuration\n",
        "out_dir = 'out_comparison'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# Data\n",
        "dataset = 'wikitext-2'\n",
        "batch_size = 12\n",
        "block_size = 256\n",
        "\n",
        "# Model\n",
        "n_layer = 4\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.1\n",
        "bias = False\n",
        "\n",
        "# Training\n",
        "learning_rate = 3e-4\n",
        "max_iters = 500\n",
        "eval_interval = 50\n",
        "eval_iters = 20\n",
        "log_interval = 10\n",
        "\n",
        "# MoAS specific\n",
        "load_balance_weight = 0.01\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load data\n",
        "def get_data():\n",
        "    data_dir = os.path.join('data', dataset)\n",
        "    input_file_path = os.path.join(data_dir, 'input.txt')\n",
        "\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    # Character-level tokenization\n",
        "    chars = sorted(list(set(data)))\n",
        "    vocab_size = len(chars)\n",
        "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "    train_ids = encode(data)\n",
        "    n = len(train_ids)\n",
        "    train_data = np.array(train_ids[:int(n*0.9)], dtype=np.uint16)\n",
        "    val_data = np.array(train_ids[int(n*0.9):], dtype=np.uint16)\n",
        "    return train_data, val_data, vocab_size\n",
        "\n",
        "train_data, val_data, vocab_size = get_data()\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "def train_model(attention_type, name):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training {name} ({attention_type})\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Create model\n",
        "    config = GPTConfig(\n",
        "        block_size=block_size,\n",
        "        vocab_size=vocab_size,\n",
        "        n_layer=n_layer,\n",
        "        n_head=n_head,\n",
        "        n_embd=n_embd,\n",
        "        dropout=dropout,\n",
        "        bias=bias,\n",
        "        attention_type=attention_type\n",
        "    )\n",
        "\n",
        "    model = GPT(config)\n",
        "    model.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = model.configure_optimizers(\n",
        "        weight_decay=0.1,\n",
        "        learning_rate=learning_rate,\n",
        "        betas=(0.9, 0.99),\n",
        "        device_type=device\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    results = {\n",
        "        'iters': [],\n",
        "        'train_loss': [],\n",
        "        'val_loss': []\n",
        "    }\n",
        "\n",
        "    X, Y = get_batch('train')\n",
        "    t0 = time.time()\n",
        "\n",
        "    for iter_num in range(max_iters + 1):\n",
        "        # Evaluation\n",
        "        if iter_num % eval_interval == 0:\n",
        "            losses = estimate_loss(model)\n",
        "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "            results['iters'].append(iter_num)\n",
        "            results['train_loss'].append(losses['train'].item())\n",
        "            results['val_loss'].append(losses['val'].item())\n",
        "\n",
        "        if iter_num == max_iters:\n",
        "            break\n",
        "\n",
        "        # Forward\n",
        "        logits, loss = model(X, Y)\n",
        "\n",
        "        # Add load balancing loss for MoAS\n",
        "        if attention_type == 'moas':\n",
        "            lb_loss = model.get_load_balancing_loss(X)\n",
        "            loss = loss + load_balance_weight * lb_loss\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Fetch next batch\n",
        "        X, Y = get_batch('train')\n",
        "\n",
        "        # Logging\n",
        "        if iter_num % log_interval == 0:\n",
        "            t1 = time.time()\n",
        "            dt = t1 - t0\n",
        "            t0 = t1\n",
        "            lossf = loss.item()\n",
        "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
        "\n",
        "    print(f\"\\n✓ {name} training completed!\")\n",
        "    return results\n",
        "\n",
        "# Train all three variants\n",
        "results_baseline = train_model('baseline', 'Baseline MHA')\n",
        "results_static = train_model('static_moas', 'Static MoAS')\n",
        "results_moas = train_model('moas', 'Dynamic MoAS (Routed)')\n",
        "\n",
        "# Save results\n",
        "import pickle\n",
        "with open(os.path.join(out_dir, 'comparison_results.pkl'), 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'baseline': results_baseline,\n",
        "        'static_moas': results_static,\n",
        "        'moas': results_moas\n",
        "    }, f)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Baseline MHA    - Final val loss: {results_baseline['val_loss'][-1]:.4f}\")\n",
        "print(f\"Static MoAS     - Final val loss: {results_static['val_loss'][-1]:.4f}\")\n",
        "print(f\"Dynamic MoAS    - Final val loss: {results_moas['val_loss'][-1]:.4f}\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZOGbxUrOPC8",
        "outputId": "6533cfbd-c89a-4e15-9a3c-c76962935cc3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing compare_moas.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python compare_moas.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwMjmrhaRr6N",
        "outputId": "932e88f4-a2f4-41d6-a83b-1880029d8fe2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "======================================================================\n",
            "Training Baseline MHA (baseline)\n",
            "======================================================================\n",
            "number of parameters: 7.19M\n",
            "num decayed parameter tensors: 18 with 7,284,864 parameters\n",
            "num non-decayed parameter tensors: 9 with 3,456 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 5.7558, val loss 5.7592\n",
            "iter 0: loss 5.7438, time 858.68ms\n",
            "iter 10: loss 3.3046, time 474.47ms\n",
            "iter 20: loss 2.9041, time 493.30ms\n",
            "iter 30: loss 2.7083, time 483.85ms\n",
            "iter 40: loss 2.6108, time 488.50ms\n",
            "step 50: train loss 2.5482, val loss 2.5545\n",
            "iter 50: loss 2.5501, time 1119.60ms\n",
            "iter 60: loss 2.5305, time 488.09ms\n",
            "iter 70: loss 2.5076, time 493.11ms\n",
            "iter 80: loss 2.4992, time 492.51ms\n",
            "iter 90: loss 2.4949, time 491.61ms\n",
            "step 100: train loss 2.4658, val loss 2.4768\n",
            "iter 100: loss 2.4760, time 1136.17ms\n",
            "iter 110: loss 2.4665, time 494.08ms\n",
            "iter 120: loss 2.4422, time 503.45ms\n",
            "iter 130: loss 2.4589, time 498.65ms\n",
            "iter 140: loss 2.4145, time 496.45ms\n",
            "step 150: train loss 2.4288, val loss 2.4397\n",
            "iter 150: loss 2.4478, time 1147.28ms\n",
            "iter 160: loss 2.4585, time 504.75ms\n",
            "iter 170: loss 2.4202, time 507.72ms\n",
            "iter 180: loss 2.3817, time 503.74ms\n",
            "iter 190: loss 2.3805, time 512.12ms\n",
            "step 200: train loss 2.4126, val loss 2.4098\n",
            "iter 200: loss 2.4114, time 1162.39ms\n",
            "iter 210: loss 2.4299, time 505.44ms\n",
            "iter 220: loss 2.4689, time 518.26ms\n",
            "iter 230: loss 2.4142, time 509.86ms\n",
            "iter 240: loss 2.4105, time 515.01ms\n",
            "step 250: train loss 2.3957, val loss 2.3827\n",
            "iter 250: loss 2.3904, time 1176.10ms\n",
            "iter 260: loss 2.4263, time 532.89ms\n",
            "iter 270: loss 2.3959, time 519.94ms\n",
            "iter 280: loss 2.3449, time 514.95ms\n",
            "iter 290: loss 2.3137, time 507.45ms\n",
            "step 300: train loss 2.3806, val loss 2.3853\n",
            "iter 300: loss 2.4379, time 1172.52ms\n",
            "iter 310: loss 2.3893, time 512.18ms\n",
            "iter 320: loss 2.3602, time 513.69ms\n",
            "iter 330: loss 2.3531, time 511.14ms\n",
            "iter 340: loss 2.4001, time 518.15ms\n",
            "step 350: train loss 2.3685, val loss 2.3543\n",
            "iter 350: loss 2.3326, time 1181.79ms\n",
            "iter 360: loss 2.3699, time 518.99ms\n",
            "iter 370: loss 2.3939, time 526.42ms\n",
            "iter 380: loss 2.3733, time 518.35ms\n",
            "iter 390: loss 2.3787, time 524.99ms\n",
            "step 400: train loss 2.3519, val loss 2.3575\n",
            "iter 400: loss 2.3523, time 1202.09ms\n",
            "iter 410: loss 2.3172, time 521.75ms\n",
            "iter 420: loss 2.3166, time 524.79ms\n",
            "iter 430: loss 2.3627, time 524.87ms\n",
            "iter 440: loss 2.3154, time 524.42ms\n",
            "step 450: train loss 2.3194, val loss 2.3261\n",
            "iter 450: loss 2.3100, time 1218.23ms\n",
            "iter 460: loss 2.3879, time 527.11ms\n",
            "iter 470: loss 2.3474, time 531.61ms\n",
            "iter 480: loss 2.3310, time 533.08ms\n",
            "iter 490: loss 2.3609, time 534.01ms\n",
            "step 500: train loss 2.2925, val loss 2.2940\n",
            "\n",
            "✓ Baseline MHA training completed!\n",
            "\n",
            "======================================================================\n",
            "Training Static MoAS (static_moas)\n",
            "======================================================================\n",
            "number of parameters: 10.14M\n",
            "num decayed parameter tensors: 50 with 10,233,984 parameters\n",
            "num non-decayed parameter tensors: 9 with 3,456 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 5.6864, val loss 5.6868\n",
            "iter 0: loss 5.6842, time 1179.54ms\n",
            "iter 10: loss 3.3170, time 875.35ms\n",
            "iter 20: loss 2.8519, time 879.10ms\n",
            "iter 30: loss 2.6824, time 879.96ms\n",
            "iter 40: loss 2.6212, time 883.27ms\n",
            "step 50: train loss 2.5280, val loss 2.5409\n",
            "iter 50: loss 2.5203, time 2001.72ms\n",
            "iter 60: loss 2.4944, time 888.29ms\n",
            "iter 70: loss 2.5481, time 891.04ms\n",
            "iter 80: loss 2.4693, time 890.07ms\n",
            "iter 90: loss 2.5008, time 895.34ms\n",
            "step 100: train loss 2.4740, val loss 2.4860\n",
            "iter 100: loss 2.5022, time 2042.97ms\n",
            "iter 110: loss 2.4240, time 901.78ms\n",
            "iter 120: loss 2.4240, time 910.21ms\n",
            "iter 130: loss 2.4511, time 904.19ms\n",
            "iter 140: loss 2.5079, time 911.65ms\n",
            "step 150: train loss 2.4267, val loss 2.4381\n",
            "iter 150: loss 2.4722, time 2086.48ms\n",
            "iter 160: loss 2.4495, time 905.60ms\n",
            "iter 170: loss 2.3974, time 916.94ms\n",
            "iter 180: loss 2.4188, time 921.75ms\n",
            "iter 190: loss 2.4539, time 918.14ms\n",
            "step 200: train loss 2.4084, val loss 2.4238\n",
            "iter 200: loss 2.4887, time 2108.71ms\n",
            "iter 210: loss 2.4111, time 922.82ms\n",
            "iter 220: loss 2.4515, time 920.54ms\n",
            "iter 230: loss 2.4204, time 924.82ms\n",
            "iter 240: loss 2.3956, time 928.57ms\n",
            "step 250: train loss 2.3776, val loss 2.4024\n",
            "iter 250: loss 2.4081, time 2109.08ms\n",
            "iter 260: loss 2.4089, time 918.39ms\n",
            "iter 270: loss 2.4446, time 910.85ms\n",
            "iter 280: loss 2.3766, time 913.52ms\n",
            "iter 290: loss 2.4084, time 913.22ms\n",
            "step 300: train loss 2.3707, val loss 2.3863\n",
            "iter 300: loss 2.4033, time 2072.29ms\n",
            "iter 310: loss 2.3959, time 904.99ms\n",
            "iter 320: loss 2.3474, time 903.55ms\n",
            "iter 330: loss 2.3833, time 900.88ms\n",
            "iter 340: loss 2.3819, time 894.88ms\n",
            "step 350: train loss 2.3552, val loss 2.3758\n",
            "iter 350: loss 2.3258, time 2029.92ms\n",
            "iter 360: loss 2.3953, time 889.11ms\n",
            "iter 370: loss 2.3383, time 890.50ms\n",
            "iter 380: loss 2.3644, time 889.42ms\n",
            "iter 390: loss 2.3581, time 888.67ms\n",
            "step 400: train loss 2.3498, val loss 2.3509\n",
            "iter 400: loss 2.3437, time 2002.39ms\n",
            "iter 410: loss 2.3338, time 886.21ms\n",
            "iter 420: loss 2.3080, time 885.05ms\n",
            "iter 430: loss 2.3381, time 881.14ms\n",
            "iter 440: loss 2.3589, time 885.61ms\n",
            "step 450: train loss 2.3143, val loss 2.3211\n",
            "iter 450: loss 2.3470, time 1981.28ms\n",
            "iter 460: loss 2.3170, time 880.24ms\n",
            "iter 470: loss 2.3737, time 876.92ms\n",
            "iter 480: loss 2.3031, time 875.55ms\n",
            "iter 490: loss 2.3366, time 876.82ms\n",
            "step 500: train loss 2.2928, val loss 2.3093\n",
            "\n",
            "✓ Static MoAS training completed!\n",
            "\n",
            "======================================================================\n",
            "Training Dynamic MoAS (Routed) (moas)\n",
            "======================================================================\n",
            "number of parameters: 10.29M\n",
            "num decayed parameter tensors: 58 with 10,382,592 parameters\n",
            "num non-decayed parameter tensors: 9 with 3,456 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 5.7528, val loss 5.7540\n",
            "iter 0: loss 5.7636, time 1308.30ms\n",
            "iter 10: loss 3.1884, time 1657.79ms\n",
            "iter 20: loss 2.7725, time 1666.04ms\n",
            "iter 30: loss 2.7002, time 1663.38ms\n",
            "iter 40: loss 2.6202, time 1665.92ms\n",
            "step 50: train loss 2.5258, val loss 2.5315\n",
            "iter 50: loss 2.5672, time 2797.33ms\n",
            "iter 60: loss 2.5284, time 1660.57ms\n",
            "iter 70: loss 2.5309, time 1666.23ms\n",
            "iter 80: loss 2.5019, time 1668.75ms\n",
            "iter 90: loss 2.5079, time 1690.02ms\n",
            "step 100: train loss 2.4639, val loss 2.4656\n",
            "iter 100: loss 2.4954, time 2817.39ms\n",
            "iter 110: loss 2.4626, time 1670.57ms\n",
            "iter 120: loss 2.4368, time 1667.20ms\n",
            "iter 130: loss 2.4495, time 1679.39ms\n",
            "iter 140: loss 2.4651, time 1678.60ms\n",
            "step 150: train loss 2.4416, val loss 2.4302\n",
            "iter 150: loss 2.5254, time 2847.89ms\n",
            "iter 160: loss 2.4298, time 1680.43ms\n",
            "iter 170: loss 2.4498, time 1682.50ms\n",
            "iter 180: loss 2.4388, time 1688.73ms\n",
            "iter 190: loss 2.3585, time 1688.29ms\n",
            "step 200: train loss 2.4153, val loss 2.4228\n",
            "iter 200: loss 2.4324, time 2870.00ms\n",
            "iter 210: loss 2.4531, time 1696.15ms\n",
            "iter 220: loss 2.3549, time 1693.72ms\n",
            "iter 230: loss 2.3980, time 1694.95ms\n",
            "iter 240: loss 2.3575, time 1697.04ms\n",
            "step 250: train loss 2.4037, val loss 2.4030\n",
            "iter 250: loss 2.4065, time 2883.89ms\n",
            "iter 260: loss 2.3782, time 1701.76ms\n",
            "iter 270: loss 2.3934, time 1701.99ms\n",
            "iter 280: loss 2.4055, time 1699.85ms\n",
            "iter 290: loss 2.3790, time 1700.53ms\n",
            "step 300: train loss 2.3805, val loss 2.3938\n",
            "iter 300: loss 2.3334, time 2895.10ms\n",
            "iter 310: loss 2.4016, time 1697.65ms\n",
            "iter 320: loss 2.3603, time 1703.36ms\n",
            "iter 330: loss 2.4182, time 1695.90ms\n",
            "iter 340: loss 2.4073, time 1696.34ms\n",
            "step 350: train loss 2.3590, val loss 2.3701\n",
            "iter 350: loss 2.3595, time 2874.42ms\n",
            "iter 360: loss 2.3829, time 1698.08ms\n",
            "iter 370: loss 2.3724, time 1696.36ms\n",
            "iter 380: loss 2.3327, time 1696.26ms\n",
            "iter 390: loss 2.3693, time 1691.99ms\n",
            "step 400: train loss 2.3630, val loss 2.3563\n",
            "iter 400: loss 2.3396, time 2860.57ms\n",
            "iter 410: loss 2.3761, time 1686.76ms\n",
            "iter 420: loss 2.3541, time 1685.23ms\n",
            "iter 430: loss 2.3868, time 1688.77ms\n",
            "iter 440: loss 2.4097, time 1683.51ms\n",
            "step 450: train loss 2.3352, val loss 2.3464\n",
            "iter 450: loss 2.3980, time 2857.20ms\n",
            "iter 460: loss 2.2805, time 1684.54ms\n",
            "iter 470: loss 2.2811, time 1678.98ms\n",
            "iter 480: loss 2.3334, time 1679.49ms\n",
            "iter 490: loss 2.3481, time 1682.44ms\n",
            "step 500: train loss 2.3102, val loss 2.3074\n",
            "\n",
            "✓ Dynamic MoAS (Routed) training completed!\n",
            "\n",
            "======================================================================\n",
            "FINAL RESULTS\n",
            "======================================================================\n",
            "Baseline MHA    - Final val loss: 2.2940\n",
            "Static MoAS     - Final val loss: 2.3093\n",
            "Dynamic MoAS    - Final val loss: 2.3074\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rdmfopk_Ruc0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}